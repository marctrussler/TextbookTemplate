[{"path":"index.html","id":"syllabus","chapter":"Syllabus","heading":"Syllabus","text":"Current 2024-06-14Draft version syllabus subject change.Lecture: MW 12-1:30pm (Location TBD)Dr. Marc Trusslertrussler@sas.upenn.edutrussler@sas.upenn.eduFox-Fels Hall 32 (3814 Walnut Street)Fox-Fels Hall 32 (3814 Walnut Street)Office Hours: TBDOffice Hours: TBD","code":""},{"path":"index.html","id":"course-description","chapter":"Syllabus","heading":"Course Description","text":"first step many data science sequences learn great deal work individual data sets: cleaning, tidying, merging, describing visualizing data. crucial skills data analytics, describing data set ultimate goal. ultimate goal data science make inferences world based small sample data .PSCI 1801 shifts focus goal inference. Using methodology emphasizes intuition simulation mathematics, course cover key statistical concepts probability, sampling, distributions, hypothesis testing, covariance. goal class students ultimately knowledge ability perform, customize, explain bivariate multivariate regression. Students taken PSCI-1800 basic familiarity R, including working vectors matrices, basic summary statistics, visualizations, () loops.","code":""},{"path":"index.html","id":"expectations-and-policies","chapter":"Syllabus","heading":"Expectations and policies","text":"","code":""},{"path":"index.html","id":"prerequisite-knowledge","chapter":"Syllabus","heading":"Prerequisite knowledge","text":"PSCI 1800 (formerly 107) similar R course. help us better understand nature inferential statistics, running quite lot simulations R. Students entering class working knowledge R programming language, particular know use square brackets index vectors run () loops. short refresher concepts first two weeks class.","code":""},{"path":"index.html","id":"course-slack-channel","chapter":"Syllabus","heading":"Course Slack Channel","text":"use Slack communicate class. receive invitation join channel shortly start class. One better things come pandemic use Slack classroom communications. really good tool allow us send quick informal messages individual students groups (message us). Similarly, allows collaborate students class, great place get simple questions answered. making announcements via Slack, extremely important get set .","code":""},{"path":"index.html","id":"formatattendance","chapter":"Syllabus","heading":"Format/Attendance","text":"lectures person. discussion-based class, expectation amount participation feedback. Attendance recorded, though note scored participation.","code":""},{"path":"index.html","id":"computers","chapter":"Syllabus","heading":"Computers","text":"course require students access personal computer order run statistics software. possible, please consult one instructors soon possible. Support cover course costs available (https://srfs.upenn.edu/sfs)[Student Financial Services].","code":""},{"path":"index.html","id":"academic-integrity","chapter":"Syllabus","heading":"Academic integrity","text":"expect students abide rules University follow Code Academic Integrity.1For Problem Sets: Collaboration problem sets permitted. Ultimately, however, write-code turn must creation. Please write names students worked top problem set. 2","code":""},{"path":"index.html","id":"re-grading-of-assignments","chapter":"Syllabus","heading":"Re-grading of assignments","text":"student work assessed using fair criteria uniform across class. , however, unsatisfied grade received particular assignment (beyond simple clerical errors), can request re-grade using following protocol. First, may send grade complaints requests re-grades least 24 hours graded assignment returned . , must document specific grievances writing submitting PDF Word Document teaching staff. document explain exactly parts assignment believe mis-graded, provide documentation answers correct.re-score entire assignment (including portions grievances), new score one receive assignment (even lower original score).","code":""},{"path":"index.html","id":"late-policy","chapter":"Syllabus","heading":"Late policy","text":"Notwithstanding everything : exceptions policies made health reasons, extraordinary family circumstances, religious holidays. teaching staff extremely reasonable lenient, long discuss us potential issues *} deadline.","code":""},{"path":"index.html","id":"assessment-and-grading","chapter":"Syllabus","heading":"Assessment and grading","text":"assignments graded anonymously. Please hand assignments Canvas student number, name.TBD","code":""},{"path":"index.html","id":"computing","chapter":"Syllabus","heading":"Computing","text":"use R class, can download free https://www.r-project.org/. R completely open source almost endless set resources online. Virtually data science job apply nowadays require background R programming.R language use, RStudio free program makes considerably easier work R. installing R, install RStudio https://www.rstudio.com. Please R RStudio installed end first week classes.’re trouble installing either program, detailed installation instructions course Canvas page.","code":""},{"path":"index.html","id":"textbook","chapter":"Syllabus","heading":"Textbook","text":"one mandatory textbook course two optional:Data Analysis Social Science: Friendly Practical Introduction. Elena Llaudet & Kosuke Imai. (Mandatory).\nchosen book really good job weaving basics statistics use R. Generally speaking assigned readings book slightly less technical class notes. book available bookstore Amazon. one addition, sure get (way cheaper) paperback version.\nData Analysis Social Science: Friendly Practical Introduction. Elena Llaudet & Kosuke Imai. (Mandatory).chosen book really good job weaving basics statistics use R. Generally speaking assigned readings book slightly less technical class notes. book available bookstore Amazon. one addition, sure get (way cheaper) paperback version.Quantitative Social Science: Introduction. Kosuke Imai.\noriginal, graduate level, textbook Llaudet Imai textbook based . chapters largely , textbook much math intensive. included equivalent readings (labeled QSS) want go greater detail. readings completely optional.\nQuantitative Social Science: Introduction. Kosuke Imai.original, graduate level, textbook Llaudet Imai textbook based . chapters largely , textbook much math intensive. included equivalent readings (labeled QSS) want go greater detail. readings completely optional.Statistics: Fourth Edition. Freedman, Pisani, Purves. (Optional).\ntextbook slightly conversational intuitive approach, incorporate lessons R. book mandatory really like style common-sense explanations book. ’s great companion around.\nStatistics: Fourth Edition. Freedman, Pisani, Purves. (Optional).textbook slightly conversational intuitive approach, incorporate lessons R. book mandatory really like style common-sense explanations book. ’s great companion around.","code":""},{"path":"index.html","id":"class-schedule","chapter":"Syllabus","heading":"Class Schedule","text":"","code":""},{"path":"index.html","id":"week-1-august-28-no-monday-class","chapter":"Syllabus","heading":"Week 1: August 28 (No Monday Class)","text":"population point.Excerpt Mlodinow (Canvas).","code":""},{"path":"index.html","id":"week-2-no-monday-class---september-4","chapter":"Syllabus","heading":"Week 2: (No Monday class) - September 4","text":"R ReviewLlaudet & Imai 1","code":""},{"path":"index.html","id":"week-3-september-9---september-11","chapter":"Syllabus","heading":"Week 3: September 9 - September 11","text":"R Review/Start probabilityLlaudet & Imai 6.1,6.2,6.7(QSS 4.11, 6.1)September 10: course selection period ends","code":""},{"path":"index.html","id":"week-4-september-16---september-18","chapter":"Syllabus","heading":"Week 4: September 16 - September 18","text":"Conditional probability independence(QSS 6.3)","code":""},{"path":"index.html","id":"week-5-september-23---september-25","chapter":"Syllabus","heading":"Week 5: September 23 - September 25","text":"Random Variables : DiscreteLlaudet & Imai 6.4.1(QSS 6.3)Problem Set 1 Due Wednesday 7pm.","code":""},{"path":"index.html","id":"week-6-september-30---october-2","chapter":"Syllabus","heading":"Week 6: September 30 - October 2","text":"Random Variables II: ContinuousLlaudet & Imai 6.4.2-6.4.4(QSS 6.4)","code":""},{"path":"index.html","id":"week-7october-7---october-9","chapter":"Syllabus","heading":"Week 7:October 7 - October 9","text":"Sampling confidence intervalsLlaudet & Imai 6.5.1,6.5.2(QSS 7.1)October 7: Drop period endsProblem Set 2 Due Wednesday 7pm.","code":""},{"path":"index.html","id":"week-8-october-14---october-16","chapter":"Syllabus","heading":"Week 8: October 14 - October 16","text":"ReviewFirst Midterm Exam period Wednesday class period..","code":""},{"path":"index.html","id":"week-9-october-21---october-23","chapter":"Syllabus","heading":"Week 9: October 21 - October 23","text":"Standard error mean/Field TripLlaudet & Imai 6.5.3On October 23rd take class field trip NBC News Decision Desk. (Tentative.)October 25: Grade type change deadline.","code":""},{"path":"index.html","id":"week-10-october-28---october-30","chapter":"Syllabus","heading":"Week 10: October 28 - October 30","text":"Hypothesis Tests PowerLlaudet & Imai 7.1 7.3 7.4(QSS 7.2)Problem Set 3 Due Wednesday 7pm.","code":""},{"path":"index.html","id":"week-11-november-4---november-6","chapter":"Syllabus","heading":"Week 11: November 4 - November 6","text":"Two continuous variables covariationLlaudet & Imai 3.5(QSS 3.6)November 4: Withdrawal deadline","code":""},{"path":"index.html","id":"week-12-november-11---november-13","chapter":"Syllabus","heading":"Week 12: November 11 - November 13","text":"Correlation bivariate regressionnLlaudet & Imai 4.3(QSS 4.2)Problem Set 4 Due Wednesday 7pm.","code":""},{"path":"index.html","id":"week-13-november-18-november-20","chapter":"Syllabus","heading":"Week 13: November 18 – November 20","text":"Multivariate Regression ILlaudet & Imai 2.1-2.4","code":""},{"path":"index.html","id":"week-14-november-25---no-wednesday-class","chapter":"Syllabus","heading":"Week 14: November 25 - (No Wednesday Class)","text":"Multivariate regression IILlaudet & Imai 5.1-5.5","code":""},{"path":"index.html","id":"week-15-december-2--december-4","chapter":"Syllabus","heading":"Week 15: December 2- December 4","text":"Interaction regressionExcerpt Kam Franzese (Canvas)Problem Set 5 Due Wednesday 7pm.","code":""},{"path":"index.html","id":"week-16-december-9","chapter":"Syllabus","heading":"Week 16: December 9","text":"Prediction regressionLlaudet & Imai 4.5-4.6(QSS 7.3.1,7.3.2)","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"taken courses data science statistics doubt gained good number skills work data: cleaning, tidying, merging, describing visualizing data. may also learned (tried learn) confusing set ideas hypothesis tests, confidence intervals, p-values. first set skills can complicated exciting learn (cover skills PSCI1800 PSCI3800), hypothesis testing can feel like afterthought.point class, call Introduction Inferential Statistics, push second set skills – loosely set skills allow us infer truths population samples data – forefront. Indeed, want gain class knowledge process inference actually entire point statistics. Everything service broader point, everything know think things way isn’t even really statistics.put shorter: defines statistics making conclusions populations sample data. (infer things populations samples, hence title class.)mean “Population”? pretty vague term, indeed use encompass really wide range things.Sometimes mean obvious: politics often take random samples thousand people help us understand opinions attitudes entire United States. case ’s quite clear sample random sample people population adult public United States.lots things day--day lives “samples” population, often just don’t consider .example: might interested learning revenue potential small business, months finances data. goal use sample data (may may perfectly representative!) better understand true earning potential business . case “population” simply true earning potential business.sports inclined can think performance baseball hitter certain stretch time “Sample” population true hitting potential.Another way conceptualize “population” think “data generating process”. (Indeed, primary way think ). idea “population” fits well example public opinion poll situations like business earnings baseball hitting. think “data generating process” think fundamental truth producing data, deterministic way.statistics, look make probabilistic inferences samples unknowable truths populations/DGPs. never precisely know earning potential business, many Americans support presidential candidate, level hitting skill baseball player . can, however, form estimates sample data estimate level uncertainty degree answer represents truth.motivating example:going look data 2020 National Election Study (NES), interviewed just 8000 Americans run 2020 election.type question often asked survey’s like “feeling thermometer”, individuals asked rate certain groups politicians scale 0 100, 0 means respondents feels “cool” towards object, 100 means respondent feels “warm”.looks first six rows data, focusing feeling thermometers Trump Biden. first individual rated Trump 100 Biden 0; second individual rated candidates 0; third rated Trump 0 Biden 65 – go rows.Note, throughout textbook use rio package import data. data textbook hosted github, can loaded using provided URLs.likely skills already may help summarize sample data looks like. likely learned previous classes, example, look distribution responses feeling thermometer question President Trump, represented density plot (can think histogram continuous variable).can see data lot density around 0, lot density 80 100. mean? means sample lot individuals like Donald Trump (give rating 0) lot individuals fairly warm view Donald Trump (give rating near 100). answers around 50, many individuals ambivalent feelings towards former President.can also summarize distribution variable sample summary statistics. mean – simple average – variable, example, 40.44. standard deviation – can thought much individual deviates mean, average – 40.31.problem ’s care . care , example Trump feeling thermometer, voters feel Trump, just ones sample. true average feeling towards Trump amongst voters? spread opinion? opinions Democrats Republicans differ? questions immediately obviously answered data front us.tempting say: well sample data population isn’t answer population? Isn’t good enough get sample, report ’s sample, assume ’s population?Let’s think way: took another sample 8000 Americans plot look exactly ? get exact mean standard deviation? Obviously wouldn’t get exactly numbers… can ask: well much expect mean vary? possible another sample might get 45? 60? 0?Let’s return baseball example think . talked series -bats “sample” population (Data Generating Process) hitters true batting ability. (“-bat” one “turn” hitter get opportunity face pitcher try get hit.)Pretend know batter’s true ability get hit 35% -bats (.350 batting average). following code simulates 10000 bats batter. Don’t worry nature code right now.question want understand : take sample data necessarily reveal truth batter? let’s look different sequences 20--bats (4 games worth). Afterall, 4 baseball games lot baseball! able determine whether someone good bad hitter time.x-axis graph time, y-axis batting average. Every point line 20--bat average. want know: degree sample data sufficient learn something population/DGP. Remember: truth batter gets hit 35% time (.350 batting average).Looking line sometimes batter right .350, often . Indeed, 4 game stretches batter hitting .100, 4 game stretches hitting .600, just random chance.heuristic “whatever sample says believe population” going wrong lot looking baseball player 4 games. inherent randomness data generating process means make good conclusions skill player.clear need ability measure express degree think can make conclusions truthHere’s another example real data. can go terms feeling thermometer look relationship age feelings towards Trump. data represented scatterplot:raw data bit harder parse. Individuals likely anchor certain responses using feeling thermometer (0,10,20 etc.) continuous variation y-axis. , people age wide variety opinions Trump: 20 year olds love , 20 year olds hate ; 80 year olds3 love , 80 year olds hate .can describe data? method focus class Ordinary Least Squares (OLS) regression. red line figure represents OLS estimate line best represents data. can use R describe line:learn lot numbers mean, short telling us. red line y-intercept 27.64. means age 0 (possible!) average feeling towards Trump 27.64. slope line .24. means age increases one year Trump thermometer increases .24 units. words, data positive relationship age feelings towards Trump. checks given know politics., pretty limited interest describing happening data particular. real object interest degree age feelings towards Trump related population. look row variable age, rest information allows us understand relationship whole population. Things like “Standard Error”, “T-Value”, “p-score” ways come understand ability make inferences whole population based information contained one sample.possible? can possibly learn something eligible voters United States sample thousand people? reason explained detail throughout course, basic intuition described briefly . Statisticians determined run survey , , , hundreds--thousands times; sample record estimate interest (mean, regression coefficient), resulting distribution estimates extremely predictable.quick simulation looks like. code make lot sense class goes . Try focus code now, just intuition giving.can use R create fake data can run regression:Looking resulting data, pretty clear positive relationship two variables, x gets higher, expect y get higher. Looking output regression see slope around 4. simply means, data, x increases one unit y increases 4 units, average.Like ANES data, know just one possible dataset many datasets . Unlike ANES data, know population/data generating process , can just actually see happens create new samples 1000 run regression. one additional sample pulled exact population. make point clear, original data points regression line displayed grey, new data regression line orange.can see new orange data points identical grey data points. new data. new data still reveal underlying relationship. data points little bit different, regression line also little bit different. Looking output regression can see slope coefficient slightly different got first sample.Now let’s go absolutely crazy. Let’s generate 5000 additional samples. , going make 5000 independent samples 1000 people. similar re-ran ANES 5000 times. ’m going re-sample population 5000 times. time ’m going run regression run new data save slope coefficient.Displayed grey 5000 different regression lines 5000 different samples stacked top one another. little bit different speak relationship population.distribution coefficients look like? understand can use density plot. density plot like continuous histogram. x-axis shows posisble values particular variable y-axis shows common value . density plot coefficients 5000 samples looks like:distribution 5000 coefficients, come learn called sampling distribution bell curve. Indeed, actually special type bell curve called normal distribution (’m sure ’ve heard ).summarize: ability magically re-sample population many times, time calculate statistic (, regression coefficient) get normal distribution coefficients.magic/mind-bending part . Let’s look first regression result first sample:Looking line x.prime see something called “Std. Error”. truly magic number contains information need determine sampling distribution woulld looke like generated 5000 (really, infinite number samples).prove , can use R draw normal distribution top sampling distribution using information first regression table:distribution identical. Let repeat ’s happening . black line distribution 5000 regression coefficients. real world, can’t just generate 5000 regression coefficients! orange line predict sampling distribution look like based results one sample. words: don’t things 5000 times! get distribution just one sample!can guess great deal accuracy distribution statistics repeated sampling , able determine likely unlikely result achieved different scenarios (call null hypothesis testing). learn critical question asked statistics : nothing going , relationship population, likely get result seeing sample?gobblygook right now, worry! big thing take away introduction following: goal statistics understand population, simply describe sample front us. gain insight , imagine calculating statistic interest many, many times. resulting distribution knowable, calculable thing, allows us determine special statistic interest .","code":"\nlibrary(rio)\nanes <- import(file=\"https://github.com/marctrussler/IIS/raw/main/Data/ANES2020Clean.csv\")\n#Examine the first 6 rows of data for these variables\nhead(anes[c(\"therm.trump\",\"therm.biden\")])\n#>   therm.trump therm.biden\n#> 1         100           0\n#> 2           0           0\n#> 3           0          65\n#> 4          15          70\n#> 5          85          15\n#> 6           0          85\n#Plot the density of the Trump feeling thermometer\nplot(density(anes$therm.trump,na.rm=T), xlim=c(0,100), main=\"Distribution of Trump Themometer Ratings\")\nabline(v=mean(anes$therm.trump,na.rm=T), lty=2)\nlegend(\"topright\", c(paste(\"Mean = \",round(mean(anes$therm.trump,na.rm=T),2 )),\n                     paste(\"SD = \",round(sd(anes$therm.trump,na.rm=T),2 )) ), lty=2, col=c(\"Black\", \"White\"))\n#Generate 1000 at-bats for a .350 hitter\nset.seed(19104)\nhits <- rbinom(10000,1, .35)\n#Use a loop to calculate the moving 20-at-bat batting average for this batter\nmoving.average <- NA\n\nfor(i in 1:9981){\n moving.average[i] <-  mean(hits[i:(i+19)])\n}\n\nplot(1:500, moving.average[1:500], type=\"l\", axes=F,\n     xlab=\"\", ylab=\"Batting Average for Previous 20 Games\", cex=.5)\nabline(h=.35, lty=2)\naxis(side=2, las=2)\naxis(side=1)\n#Plot Feeling Thermometer against age\nplot(anes$age, anes$therm.trump)\nabline(lm(anes$therm.trump ~ anes$age), col=\"firebrick\", lwd=2)\n#Run a regression predicing feelings towards Trump by age\nm <- lm(therm.trump ~ age, data=anes)\nsummary(m)\n#> \n#> Call:\n#> lm(formula = therm.trump ~ age, data = anes)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -47.15 -38.13 -11.79  41.26  67.97 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 27.64585    1.43952  19.205   <2e-16 ***\n#> age          0.24380    0.02661   9.162   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 40.11 on 7725 degrees of freedom\n#>   (553 observations deleted due to missingness)\n#> Multiple R-squared:  0.01075,    Adjusted R-squared:  0.01062 \n#> F-statistic: 83.95 on 1 and 7725 DF,  p-value: < 2.2e-16\n#Creation of fake data:\nset.seed(19104)\nx.prime <- rnorm(1000,mean=3, sd=6)\ny.prime <- 3*rnorm(1000) + 4*x.prime + rnorm(1000, 0, 30)\nplot(x.prime,y.prime)\nabline(lm(y.prime ~ x.prime), col=\"firebrick\", lwd=2)\nsummary(lm(y.prime~x.prime))\n#> \n#> Call:\n#> lm(formula = y.prime ~ x.prime)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -87.604 -18.997  -0.583  19.131  93.628 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   0.2532     1.0607   0.239    0.811    \n#> x.prime       3.8715     0.1557  24.858   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 29.74 on 998 degrees of freedom\n#> Multiple R-squared:  0.3824, Adjusted R-squared:  0.3818 \n#> F-statistic: 617.9 on 1 and 998 DF,  p-value: < 2.2e-16\nx <- rnorm(1000,mean=3, sd=6)\ny <- 3*rnorm(1000) + 4*x + rnorm(1000, 0, 30)\nplot(x.prime,y.prime, col=\"gray80\")\nabline(lm(y.prime ~ x.prime), col=\"gray30\", lwd=2)\npoints(x,y, col=\"darkorange\")\nabline(lm(y~x), col=\"darkorange\", lwd=2)\nsummary(lm(y~x))\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -104.107  -21.801   -0.963   21.259   91.605 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.5727     1.1068  -0.517    0.605    \n#> x             3.6858     0.1651  22.329   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 30.62 on 998 degrees of freedom\n#> Multiple R-squared:  0.3331, Adjusted R-squared:  0.3325 \n#> F-statistic: 498.6 on 1 and 998 DF,  p-value: < 2.2e-16\ncoefs <- rep(NA, 5000)\n\nplot(x.prime,y.prime, type=\"n\")\nabline(lm(y.prime ~ x.prime), col=\"gray30\", lwd=2)\nfor(i in 1:5000){\nx <- rnorm(1000,mean=3, sd=6)\ny <- 3*rnorm(1000) + 4*x + rnorm(1000, 0, 30)\nm <- lm(y ~ x)\ncoefs[i] <- coefficients(m)[\"x\"]\nabline(m, col=\"gray60\")\n}\nabline(lm(y.prime ~ x.prime), col=\"firebrick\", lwd=2)\nplot(density(coefs), main=\"Sampling Distribution of Beta Coefficients\")\nabline(v=4, col=\"firebrick\", lwd=2)\nsummary(lm(y.prime ~ x.prime))\n#> \n#> Call:\n#> lm(formula = y.prime ~ x.prime)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -87.604 -18.997  -0.583  19.131  93.628 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   0.2532     1.0607   0.239    0.811    \n#> x.prime       3.8715     0.1557  24.858   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 29.74 on 998 degrees of freedom\n#> Multiple R-squared:  0.3824, Adjusted R-squared:  0.3818 \n#> F-statistic: 617.9 on 1 and 998 DF,  p-value: < 2.2e-16\nplot(density(coefs), main=\"Sampling Distribution of Beta Coefficients\")\nx <- seq(3,5,.001)\n#Note the use of  0.1557, which is from the regression table above!\npoints(x,dnorm(x, mean=4, sd= 0.1557), type=\"l\", col=\"darkorange\")\n\nabline(v=4, col=\"firebrick\", lwd=2)"}]
